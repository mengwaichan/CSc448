{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-12-19T19:14:05.337029Z","iopub.status.busy":"2023-12-19T19:14:05.336669Z","iopub.status.idle":"2023-12-19T19:14:05.344332Z","shell.execute_reply":"2023-12-19T19:14:05.343436Z","shell.execute_reply.started":"2023-12-19T19:14:05.336995Z"}},"source":["---\n","\n","# Georgios_Ioannou\n"]},{"cell_type":"markdown","metadata":{},"source":["## Copyright © 2023 by Georgios Ioannou\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<h1 align=\"center\"> Text Emotion System Sentiment Analysis </h1>\n","<h2 align=\"center\"> TESSA </h2>\n","\n","In this notebook, we will be classifying emotion based on text documents. The dataset we will be using is called:\n","\n","<p style=\"text-align: center;\"><a href=\"https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\">Emotions Dataset for NLP</a></p>\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<h2 align=\"center\"> Remember our main steps motto \"ISBE\" </h2>\n","\n","<h3 align=\"center\"> Main Steps when building a Machine Learning Model </h3>\n","\n","1. **I** - `Inspect and explore data`\n","2. **S** - `Select and engineer features`\n","3. **B** - `Build and train model`\n","4. **E** - `Evaluate model`\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<h2 align='center'> GPU Information </h2>\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:27.660014Z","iopub.status.busy":"2023-12-20T08:15:27.659641Z","iopub.status.idle":"2023-12-20T08:15:28.644849Z","shell.execute_reply":"2023-12-20T08:15:28.643753Z","shell.execute_reply.started":"2023-12-20T08:15:27.659979Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Dec 20 08:15:28 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<h2 align='center'> Libraries </h2>\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:28.647602Z","iopub.status.busy":"2023-12-20T08:15:28.647274Z","iopub.status.idle":"2023-12-20T08:15:34.349236Z","shell.execute_reply":"2023-12-20T08:15:34.348325Z","shell.execute_reply.started":"2023-12-20T08:15:28.647575Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /kaggle/working/...\n","Archive:  /kaggle/working/corpora/wordnet.zip\n","   creating: /kaggle/working/corpora/wordnet/\n","  inflating: /kaggle/working/corpora/wordnet/lexnames  \n","  inflating: /kaggle/working/corpora/wordnet/data.verb  \n","  inflating: /kaggle/working/corpora/wordnet/index.adv  \n","  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n","  inflating: /kaggle/working/corpora/wordnet/index.verb  \n","  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n","  inflating: /kaggle/working/corpora/wordnet/data.adj  \n","  inflating: /kaggle/working/corpora/wordnet/index.adj  \n","  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n","  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n","  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n","  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n","  inflating: /kaggle/working/corpora/wordnet/README  \n","  inflating: /kaggle/working/corpora/wordnet/index.sense  \n","  inflating: /kaggle/working/corpora/wordnet/data.noun  \n","  inflating: /kaggle/working/corpora/wordnet/data.adv  \n","  inflating: /kaggle/working/corpora/wordnet/index.noun  \n","  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Import libraries.\n","\n","# Use inline so our visualizations display in notebook.\n","\n","\n","%matplotlib inline\n","\n","\n","import matplotlib.pyplot as plt   # Data visualization.\n","import nltk                       # Natural Language Processing.\n","import numpy as np                # Data wrangling.\n","import os                         # Manipulate operating system interfaces.\n","import pandas as pd               # Data handling.\n","pd.set_option('display.max_colwidth', None)\n","import pickle                     # Python object serialization.\n","import plotly.express as px       # Data visualization\n","import plotly.graph_objects as go # Data visualization\n","import re                         # Regular expression operations.\n","import seaborn as sns             # Data visualization.\n","import subprocess                 # To download nltk wordnet in Kaggle.\n","sns.set()\n","import warnings                   # Ignore all warnings.\n","warnings.filterwarnings('ignore')\n","\n","\n","from nltk.stem import WordNetLemmatizer # Lemmatize using WordNet's built-in morphy function.\n","from nltk.stem import PorterStemmer     # Remove morphological affixes from words, leaving only the word stem.\n","from nltk.corpus import stopwords       # Remove stopwaords.\n","from nltk import word_tokenize          # Tokenize.\n","from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts.\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, multilabel_confusion_matrix, precision_score, recall_score # Evaluation metrics.\n","from sklearn.model_selection import train_test_split     # Eplit data in training/validating/testing.\n","from sklearn.naive_bayes import MultinomialNB            # Multinomial Naive Bayes classifier.\n","from sklearn.preprocessing import LabelEncoder           # Encode target labels with value between 0 and n_classes-1.\n","from tensorflow.keras.callbacks import EarlyStopping     # Stop training when a monitored metric has stopped improving.\n","from tensorflow.keras.callbacks import ReduceLROnPlateau # Reduce learning rate when a metric has stopped improving.\n","from tensorflow.keras.layers import Activation, BatchNormalization, Bidirectional, Concatenate, Conv1D, Dense, Dropout, Embedding, GlobalMaxPooling1D, LSTM, MaxPooling1D, ReLU # Keras layers API.\n","from tensorflow.keras.models import Model, Sequential # Model achitecture.\n","from tensorflow.keras.optimizers import Adam         # Adam optimizer.\n","from tensorflow.keras.preprocessing.sequence import pad_sequences # Transformsa list of sequences into a 2D Numpy array.\n","from tensorflow.keras.preprocessing.text import Tokenizer         # Vectorize a text corpus.\n","from tensorflow.keras.utils import plot_model                     # Visualize the model and save it.\n","from tensorflow.keras.utils import to_categorical                 # Converts a class vector (integers) to binary class matrix.\n","\n","\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = 'unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora'\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","    \n","\n","from nltk.corpus import wordnet\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## #2 Select And Engineer Features\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Preprocess The Data Using NLTK\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:37.668460Z","iopub.status.busy":"2023-12-20T08:15:37.668188Z","iopub.status.idle":"2023-12-20T08:15:37.673871Z","shell.execute_reply":"2023-12-20T08:15:37.673117Z","shell.execute_reply.started":"2023-12-20T08:15:37.668436Z"},"trusted":true},"outputs":[],"source":["stop_words = set(stopwords.words(\"english\"))\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:37.675202Z","iopub.status.busy":"2023-12-20T08:15:37.674900Z","iopub.status.idle":"2023-12-20T08:15:37.688856Z","shell.execute_reply":"2023-12-20T08:15:37.688016Z","shell.execute_reply.started":"2023-12-20T08:15:37.675178Z"},"trusted":true},"outputs":[],"source":["# Cleaner class is responsible for cleaning the documents using its pipeline function.\n","\n","\n","class Cleaner:\n","    def __init__(self):\n","        pass\n","\n","    # 1. Make a function that makes all text lowercase.\n","\n","    def make_lowercase(self, input_string):\n","        input_string = input_string.split()\n","        input_string = [y.lower() for y in input_string]\n","        return \" \".join(input_string)\n","\n","    # 2. Make a function that removes all stopwords.\n","\n","    def remove_stopwords(self, input_string):\n","        input_string = [i for i in str(input_string).split() if i not in stop_words]\n","        return \" \".join(input_string)\n","\n","    # 3. Make a function that removes all numbers.\n","\n","    def remove_numbers(self, input_string):\n","        input_string = \"\".join([i for i in input_string if not i.isdigit()])\n","        return input_string\n","\n","    # 4. Make a function that removes all punctuation.\n","\n","    def remove_punctuation(self, input_string):\n","        input_string = re.sub(\n","            \"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"),\n","            \" \",\n","            input_string,\n","        )\n","        input_string = input_string.replace(\n","            \"؛\",\n","            \"\",\n","        )\n","        input_string = re.sub(\"\\s+\", \" \", input_string)\n","        input_string = \" \".join(input_string.split())\n","        return input_string.strip()\n","\n","    # 5. Make a function that removes all urls.\n","\n","    def remove_urls(self, input_string):\n","        url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n","        return url_pattern.sub(r\"\", input_string)\n","\n","    # 6. Make a function for lemmatization.\n","\n","    def lemmatization(self, input_string):\n","        lemmatizer = WordNetLemmatizer()\n","        input_string = input_string.split()\n","        input_string = [lemmatizer.lemmatize(y) for y in input_string]\n","        return \" \".join(input_string)\n","\n","    # 7. Make a function that breaks words into their stem words.\n","\n","    def stem_words(self, input_string):\n","        porter = PorterStemmer()\n","        words = word_tokenize(input_string)\n","        valid_words = []\n","\n","        for word in words:\n","            stemmed_word = porter.stem(word)\n","            valid_words.append(stemmed_word)\n","\n","        input_string = \" \".join(valid_words)\n","\n","        return input_string\n","\n","    # 8. Make a pipeline function that applies all the text processing functions you just built.\n","\n","    def pipeline(self, input_string):\n","        input_string = self.make_lowercase(input_string)  # 1.\n","        input_string = self.remove_stopwords(input_string)  # 2.\n","        input_string = self.remove_numbers(input_string)  # 3.\n","        input_string = self.remove_punctuation(input_string)  # 4.\n","        input_string = self.remove_urls(input_string)  # 5.\n","        input_string = self.lemmatization(input_string)  # 6.\n","        #         input_string = self.stem_words(input_string)         # 7.\n","        return input_string"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:37.690428Z","iopub.status.busy":"2023-12-20T08:15:37.690172Z","iopub.status.idle":"2023-12-20T08:15:41.449926Z","shell.execute_reply":"2023-12-20T08:15:41.448931Z","shell.execute_reply.started":"2023-12-20T08:15:37.690405Z"},"trusted":true},"outputs":[],"source":["# Clean/Normalize the documents.\n","\n","cleaner = Cleaner()\n","\n","combined_df[\"document_clean\"] = combined_df[\"document\"]\n","combined_df[\"document_clean\"] = combined_df[\"document\"].apply(cleaner.pipeline)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.451665Z","iopub.status.busy":"2023-12-20T08:15:41.451293Z","iopub.status.idle":"2023-12-20T08:15:41.461165Z","shell.execute_reply":"2023-12-20T08:15:41.460273Z","shell.execute_reply.started":"2023-12-20T08:15:41.451632Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ORIGINAL DOCUMENT:\n","\n","\n"]},{"data":{"text/plain":["0                                                  i didnt feel humiliated\n","0    im feeling quite sad and sorry for myself but ill snap out of it soon\n","0              im feeling rather rotten so im not very ambitious right now\n","Name: document, dtype: object"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#  Print the first original document.\n","\n","print(\"ORIGINAL DOCUMENT:\\n\\n\")\n","combined_df[\"document\"][0]"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.462713Z","iopub.status.busy":"2023-12-20T08:15:41.462370Z","iopub.status.idle":"2023-12-20T08:15:41.493832Z","shell.execute_reply":"2023-12-20T08:15:41.492981Z","shell.execute_reply.started":"2023-12-20T08:15:41.462681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","CLEANED DOCUMENT:\n","\n","\n"]},{"data":{"text/plain":["0                          didnt feel humiliated\n","0       im feeling quite sad sorry ill snap soon\n","0    im feeling rather rotten im ambitious right\n","Name: document_clean, dtype: object"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["#  Print the first cleaned document.\n","\n","print(\"\\nCLEANED DOCUMENT:\\n\\n\")\n","combined_df[\"document_clean\"][0]"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.496333Z","iopub.status.busy":"2023-12-20T08:15:41.494920Z","iopub.status.idle":"2023-12-20T08:15:41.517380Z","shell.execute_reply":"2023-12-20T08:15:41.516420Z","shell.execute_reply.started":"2023-12-20T08:15:41.496306Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>emotion</th>\n","      <th>document_length</th>\n","      <th>document_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i didnt feel humiliated</td>\n","      <td>sadness</td>\n","      <td>23</td>\n","      <td>didnt feel humiliated</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n","      <td>sadness</td>\n","      <td>108</td>\n","      <td>go feeling hopeless damned hopeful around someone care awake</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>im grabbing a minute to post i feel greedy wrong</td>\n","      <td>anger</td>\n","      <td>48</td>\n","      <td>im grabbing minute post feel greedy wrong</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n","      <td>love</td>\n","      <td>92</td>\n","      <td>ever feeling nostalgic fireplace know still property</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am feeling grouchy</td>\n","      <td>anger</td>\n","      <td>20</td>\n","      <td>feeling grouchy</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>i just keep feeling like someone is being unkind to me and doing me wrong and then all i can think of doing is to get back at them and the people they are close to</td>\n","      <td>anger</td>\n","      <td>163</td>\n","      <td>keep feeling like someone unkind wrong think get back people close</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>im feeling a little cranky negative after this doctors appointment</td>\n","      <td>anger</td>\n","      <td>66</td>\n","      <td>im feeling little cranky negative doctor appointment</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>i feel that i am useful to my people and that gives me a great feeling of achievement</td>\n","      <td>joy</td>\n","      <td>85</td>\n","      <td>feel useful people give great feeling achievement</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>im feeling more comfortable with derby i feel as though i can start to step out my shell</td>\n","      <td>joy</td>\n","      <td>88</td>\n","      <td>im feeling comfortable derby feel though start step shell</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>i feel all weird when i have to meet w people i text but like dont talk face to face w</td>\n","      <td>fear</td>\n","      <td>86</td>\n","      <td>feel weird meet w people text like dont talk face face w</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>19948 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                                                                                                                                 document  \\\n","0                                                                                                                                                 i didnt feel humiliated   \n","1                                                            i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n","2                                                                                                                        im grabbing a minute to post i feel greedy wrong   \n","3                                                                            i am ever feeling nostalgic about the fireplace i will know that it is still on the property   \n","4                                                                                                                                                    i am feeling grouchy   \n","...                                                                                                                                                                   ...   \n","1995  i just keep feeling like someone is being unkind to me and doing me wrong and then all i can think of doing is to get back at them and the people they are close to   \n","1996                                                                                                   im feeling a little cranky negative after this doctors appointment   \n","1997                                                                                i feel that i am useful to my people and that gives me a great feeling of achievement   \n","1998                                                                             im feeling more comfortable with derby i feel as though i can start to step out my shell   \n","1999                                                                               i feel all weird when i have to meet w people i text but like dont talk face to face w   \n","\n","      emotion  document_length  \\\n","0     sadness               23   \n","1     sadness              108   \n","2       anger               48   \n","3        love               92   \n","4       anger               20   \n","...       ...              ...   \n","1995    anger              163   \n","1996    anger               66   \n","1997      joy               85   \n","1998      joy               88   \n","1999     fear               86   \n","\n","                                                          document_clean  \n","0                                                  didnt feel humiliated  \n","1           go feeling hopeless damned hopeful around someone care awake  \n","2                              im grabbing minute post feel greedy wrong  \n","3                   ever feeling nostalgic fireplace know still property  \n","4                                                        feeling grouchy  \n","...                                                                  ...  \n","1995  keep feeling like someone unkind wrong think get back people close  \n","1996                im feeling little cranky negative doctor appointment  \n","1997                   feel useful people give great feeling achievement  \n","1998           im feeling comfortable derby feel though start step shell  \n","1999            feel weird meet w people text like dont talk face face w  \n","\n","[19948 rows x 4 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Print the combined_df Pandas dataframe with the clean documents.\n","\n","combined_df"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Define X And y\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.518930Z","iopub.status.busy":"2023-12-20T08:15:41.518574Z","iopub.status.idle":"2023-12-20T08:15:41.529293Z","shell.execute_reply":"2023-12-20T08:15:41.528368Z","shell.execute_reply.started":"2023-12-20T08:15:41.518899Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0                                                    didnt feel humiliated\n","1             go feeling hopeless damned hopeful around someone care awake\n","2                                im grabbing minute post feel greedy wrong\n","3                     ever feeling nostalgic fireplace know still property\n","4                                                          feeling grouchy\n","                                       ...                                \n","1995    keep feeling like someone unkind wrong think get back people close\n","1996                  im feeling little cranky negative doctor appointment\n","1997                     feel useful people give great feeling achievement\n","1998             im feeling comfortable derby feel though start step shell\n","1999              feel weird meet w people text like dont talk face face w\n","Name: document_clean, Length: 19948, dtype: object"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# X is our feature (document).\n","\n","X = combined_df[\"document_clean\"]\n","\n","X"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.530662Z","iopub.status.busy":"2023-12-20T08:15:41.530404Z","iopub.status.idle":"2023-12-20T08:15:41.541654Z","shell.execute_reply":"2023-12-20T08:15:41.540795Z","shell.execute_reply.started":"2023-12-20T08:15:41.530639Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["y.value_counts() =\n","\n","emotion\n","joy         6739\n","sadness     5793\n","anger       2703\n","fear        2369\n","love        1630\n","surprise     714\n","Name: count, dtype: int64\n"]}],"source":["# y is our label. What we want to predict.\n","\n","y = combined_df[\"emotion\"]\n","\n","print(\"y.value_counts() =\\n\")\n","print(y.value_counts())"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.550994Z","iopub.status.busy":"2023-12-20T08:15:41.550676Z","iopub.status.idle":"2023-12-20T08:15:41.557799Z","shell.execute_reply":"2023-12-20T08:15:41.556890Z","shell.execute_reply.started":"2023-12-20T08:15:41.550971Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0       sadness\n","1       sadness\n","2         anger\n","3          love\n","4         anger\n","         ...   \n","1995      anger\n","1996      anger\n","1997        joy\n","1998        joy\n","1999       fear\n","Name: emotion, Length: 19948, dtype: object"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3 Split Data (train_test_split)\n","\n","- Train = 80%\n","- Validation = 10%\n","- Test = 10%\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.559084Z","iopub.status.busy":"2023-12-20T08:15:41.558810Z","iopub.status.idle":"2023-12-20T08:15:41.571921Z","shell.execute_reply":"2023-12-20T08:15:41.570957Z","shell.execute_reply.started":"2023-12-20T08:15:41.559037Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train.shape  =  (15958,)\n","X_valid.shape  =  (1995,)\n","X_test.shape   =  (1995,)\n","\n","y_train.shape  =  (15958,)\n","y_valid.shape  =  (1995,)\n","y_test.shape   =  (1995,)\n"]}],"source":["# Train test split twice to get the train, validation, and test data.\n","\n","X_train, X_remain, y_train, y_remain = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","X_valid, X_test, y_valid, y_test = train_test_split(\n","    X_remain, y_remain, test_size=0.5, random_state=42\n",")\n","\n","\n","print(\"X_train.shape  = \", X_train.shape)  # 1 Dimension.\n","print(\"X_valid.shape  = \", X_valid.shape)  # 1 Dimension.\n","print(\"X_test.shape   = \", X_test.shape)  # 1 Dimension.\n","\n","print()\n","\n","print(\"y_train.shape  = \", y_train.shape)  # 1 Dimension.\n","print(\"y_valid.shape  = \", y_valid.shape)  # 1 Dimension.\n","print(\"y_test.shape   = \", y_test.shape)  # 1 Dimension."]},{"cell_type":"markdown","metadata":{},"source":["### 2.4 Label Encoder\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.573363Z","iopub.status.busy":"2023-12-20T08:15:41.573052Z","iopub.status.idle":"2023-12-20T08:15:41.587363Z","shell.execute_reply":"2023-12-20T08:15:41.586432Z","shell.execute_reply.started":"2023-12-20T08:15:41.573339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["14962     sadness\n","76            joy\n","1707      sadness\n","254         anger\n","940           joy\n","           ...   \n","11297        fear\n","11980       anger\n","5391          joy\n","860       sadness\n","15825    surprise\n","Name: emotion, Length: 15958, dtype: object\n","****************************************************************************************************\n","[4 2 4 ... 2 4 5]\n","****************************************************************************************************\n","label.classes_ = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n"]}],"source":["# Create an instance of the LabelEncoder class.\n","\n","label = LabelEncoder()\n","\n","# Print the original y_train values.\n","\n","print(y_train)\n","print(\"*\" * 100)\n","\n","# Fit the LabelEncoder on the y_train data and transform it.\n","\n","y_train = label.fit_transform(y_train)\n","\n","# Print the transformed y_train values.\n","\n","print(y_train)\n","print(\"*\" * 100)\n","\n","# Transform the y_test and y_valid data using the fitted LabelEncoder.\n","\n","y_test = label.transform(y_test)\n","y_valid = label.transform(y_valid)\n","\n","# Print the classes that the LabelEncoder has learned.\n","\n","print(\"label.classes_ =\", label.classes_)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.588700Z","iopub.status.busy":"2023-12-20T08:15:41.588451Z","iopub.status.idle":"2023-12-20T08:15:41.598796Z","shell.execute_reply":"2023-12-20T08:15:41.597829Z","shell.execute_reply.started":"2023-12-20T08:15:41.588678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[4 2 4 ... 2 4 5]\n","****************************************************************************************************\n","[[0. 0. 0. 0. 1. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," ...\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0.]\n"," [0. 0. 0. 0. 0. 1.]]\n","****************************************************************************************************\n","label.classes_ = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n"]}],"source":["# Print the original y_train values.\n","\n","print(y_train)\n","print(\"*\" * 100)\n","\n","# Convert y_train into a binary matrix representation.\n","\n","y_train = to_categorical(y_train)\n","\n","# Print the transformed y_train values.\n","\n","print(y_train)\n","print(\"*\" * 100)\n","\n","# Convert y_test and y_valid into a binary matrix representation.\n","\n","y_test = to_categorical(y_test)\n","y_valid = to_categorical(y_valid)\n","\n","# Print the classes that the LabelEncoder has learned.\n","\n","print(\"label.classes_ =\", label.classes_)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5 Tokenize\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.600106Z","iopub.status.busy":"2023-12-20T08:15:41.599804Z","iopub.status.idle":"2023-12-20T08:15:41.921712Z","shell.execute_reply":"2023-12-20T08:15:41.921016Z","shell.execute_reply.started":"2023-12-20T08:15:41.600073Z"},"trusted":true},"outputs":[],"source":["# Create an instance of the Tokenizer class.\n","\n","tokenizer = Tokenizer()\n","\n","# Fit the Tokenizer on the combined X_train and X_test data.\n","\n","tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:41.922974Z","iopub.status.busy":"2023-12-20T08:15:41.922694Z","iopub.status.idle":"2023-12-20T08:15:42.465366Z","shell.execute_reply":"2023-12-20T08:15:42.464544Z","shell.execute_reply.started":"2023-12-20T08:15:41.922951Z"},"trusted":true},"outputs":[],"source":["# Convert X_train, X_test, and X_valid into sequences of integers\n","# so that they can be input in the model.\n","\n","sequences_train = tokenizer.texts_to_sequences(X_train)\n","sequences_valid = tokenizer.texts_to_sequences(X_valid)\n","sequences_test = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:42.466908Z","iopub.status.busy":"2023-12-20T08:15:42.466618Z","iopub.status.idle":"2023-12-20T08:15:42.560437Z","shell.execute_reply":"2023-12-20T08:15:42.559700Z","shell.execute_reply.started":"2023-12-20T08:15:42.466883Z"},"trusted":true},"outputs":[],"source":["# Pad the sequences to ensure they all have the same length.\n","\n","X_train = pad_sequences(sequences_train, maxlen=256, truncating=\"pre\")\n","X_valid = pad_sequences(sequences_valid, maxlen=256, truncating=\"pre\")\n","X_test = pad_sequences(sequences_test, maxlen=256, truncating=\"pre\")"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T08:15:42.561707Z","iopub.status.busy":"2023-12-20T08:15:42.561453Z","iopub.status.idle":"2023-12-20T08:15:42.566690Z","shell.execute_reply":"2023-12-20T08:15:42.565644Z","shell.execute_reply.started":"2023-12-20T08:15:42.561685Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["vocabulary_size = 14326\n"]}],"source":["# Get the size of the vocabulary.\n","\n","vocabulary_size = len(tokenizer.index_word) + 1\n","print(\"vocabulary_size =\", vocabulary_size)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":605165,"sourceId":1085454,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
